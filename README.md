# üõ†Ô∏è Herramientas a instalar
Proyecto de backend con inteligencia artificial: Es fundamental instalar las siguientes herramientas: `Python 3.13`, `Ollama`, el modelo `deepseek-r1` y `Postman`.

## Python 3.13
**Descarga:** [Descargar Python 3.13](https://www.python.org/downloads/release/python-3130/)

**Instrucciones de instalaci√≥n:**

1. Haz clic en el enlace de descarga y selecciona el instalador adecuado para tu sistema operativo (Windows, macOS o Linux).
2. Ejecuta el archivo descargado y sigue las instrucciones del asistente de instalaci√≥n.
3. Aseg√∫rate de marcar la opci√≥n `"Add Python to PATH"` para facilitar el uso de Python desde la terminal.

## Ollama

**Descarga:** [Descargar Ollama](https://ollama.com/download)

**Instrucciones de instalaci√≥n:**

1. Descarga el archivo correspondiente para tu sistema operativo.
2. Comprime y extrae el contenido en una carpeta de tu elecci√≥n.
3. Abre la terminal y navega a la carpeta de extracci√≥n, y ejecuta el comando de instalaci√≥n personalizado proporcionado en su documentaci√≥n.

## Modelo deepseek-r

**Instrucciones de instalaci√≥n:**

1. Una vez que tengas Ollama instalado, abre la terminal.
2. Ejecuta el siguiente comando para descargar el modelo:

``` shell
ollama pull deepseek-r
```
3. Espera a que el modelo se descargue completamente antes de continuar.
## Postman

Descripci√≥n: Postman es una herramienta fundamental para probar APIs. Permite
enviar solicitudes HTTP y analizar respuestas de una manera intuitiva.

**Descarga:** [Descargar Postman](https://www.postman.com/downloads/)

**Instrucciones de instalaci√≥n:**

1. Visita el enlace y selecciona el instalador apropiado.
2. Ejecuta el archivo descargado y sigue las instrucciones del instalador.
3. Al finalizar, abre Postman y crea una cuenta gratuita para acceder a todas las funcionalidades.  

# üìÅ Estructura del Proyecto
A continuaci√≥n, se describen los pasos para configurar el proyecto y la instalaci√≥n de las librer√≠as necesarias.

### Estructura de archivos del proyecto

semana0_nombre-pasante/  
‚îÇ  
‚îú‚îÄ‚îÄ app/                # Carpeta principal del c√≥digo    
‚îÇ   ‚îú‚îÄ‚îÄ main.py         # Archivo principal para ejecutar la aplicaci√≥n  
‚îÇ           
‚îú‚îÄ‚îÄ .env                # Archivo para gestionar variables de entorno  
‚îú‚îÄ‚îÄ requirements.txt    # Archivo para las dependencias del proyecto  
‚îî‚îÄ‚îÄ README.md           # Documento explicativo del proyecto 

### Configuraci√≥n del Entorno Virtual

La utilizaci√≥n de un entorno virtual es una buena pr√°ctica en el desarrollo de
Python, ya que te permite gestionar las librer√≠as de manera independiente. Para
crear un entorno virtual, sigue los siguientes pasos:

1. Crear el entorno virtual: Ejecuta el siguiente comando en la terminal:

``` shell
python - m venv env
```
Esto crear√° un entorno virtual llamado env.

2. Activar el entorno virtual:
- Para Windows:

``` shell
.\env\Scripts\activate
```

- Para macOS o Linux:

``` shell
source env/bin/activate
```

Una vez activado, ver√°s que el nombre del entorno aparece al inicio de la l√≠nea de
tu terminal.

#### Instalaci√≥n de Librer√≠as Necesarias

Ahora que tienes tu entorno virtual configurado, es momento de instalar las
librer√≠as necesarias para el proyecto. Utiliza los siguientes comandos:

``` shell
pip install fastapi uvicorn requests python-dotenv ollama
```
### Descripci√≥n de Librer√≠as:

- **FastAPI:** Framework para construir APIs de alta eficiencia.
- **Uvicorn:** Servidor ASGI para ejecutar tu aplicaci√≥n FastAPI.
- **Requests:** Librer√≠a para realizar solicitudes HTTP.
- **Python-dotenv:** Permite trabajar con archivos .env para gestionar variables de entorno.
- **Ollama:** Herramienta para ejecutar modelos de IA localmente.


## Probar la API con Postman
#### Endpoint Preguntar

```http
  POST http://127.0.0.1:8000/preguntar
```
#### Body
```json
{
    "texto": "¬øCu√°l es la capital de Francia?"
}
``` 

| Attribute | Type     | Description                |
| :-------- | :------- | :------------------------- |
| `texto` | `string` | **Required**. |

#### Response

```json
{
    "respuesta": "Esta es una respuesta simulada para: ¬øCu√°l es la capital de Francia?"
}
``` 
  
# üîç Investigaci√≥n

## 1. ¬øQu√© es Ollama?
Ollama es una herramienta o framework dise√±ado para facilitar la ejecuci√≥n y gesti√≥n de modelos de lenguaje grandes (LLM, por sus siglas en ingl√©s) de manera local. Permite a los desarrolladores descargar, ejecutar y personalizar modelos como Llama, Mistral u otros, optimizando su uso en entornos locales o en servidores. Ollama simplifica el proceso de implementaci√≥n al proporcionar una interfaz sencilla para interactuar con estos modelos, gestionar recursos y ajustar par√°metros clave.

## 2. ¬øQu√© es FastAPI?
FastAPI es un framework moderno y r√°pido para construir APIs web en Python. Est√° dise√±ado para ser altamente eficiente y f√°cil de usar, aprovechando las ventajas de Python 3.7+ y sus caracter√≠sticas de tipado est√°tico. FastAPI se destaca por su capacidad para manejar solicitudes as√≠ncronas, lo que lo hace ideal para aplicaciones que requieren alto rendimiento, como APIs que consumen modelos de IA pesados. Adem√°s, genera autom√°ticamente documentaci√≥n interactiva (Swagger y ReDoc) basada en los endpoints definidos.

## 3. ¬øQu√© es el modelo deepseek-r1?
El modelo **deepseek-r1** es uno de los modelos de lenguaje desarrollados por DeepSeek, una empresa especializada en inteligencia artificial. Este modelo pertenece a la familia de modelos de lenguaje grandes (LLM) y est√° dise√±ado para tareas avanzadas de procesamiento del lenguaje natural (NLP), como generaci√≥n de texto, traducci√≥n, resumen y m√°s. El sufijo "r1" puede indicar una versi√≥n espec√≠fica o una iteraci√≥n del modelo, optimizada para ciertos casos de uso o mejoras en rendimiento.

## 4. Uso de peticiones con stream=True
El uso de `stream=True` en bibliotecas como `requests` de Python permite recibir respuestas de manera incremental, en lugar de esperar a que toda la respuesta est√© disponible antes de procesarla. Esto es especialmente √∫til cuando se trabaja con APIs que generan datos en tiempo real o cuando se manejan respuestas grandes que podr√≠an sobrecargar la memoria. Por ejemplo:

```python
import requests

# Realizar una solicitud con stream=True
response = requests.get('https://api.ejemplo.com/stream', stream=True)

# Verificar si la solicitud fue exitosa
if response.status_code == 200:
    # Iterar sobre los fragmentos de la respuesta
    for chunk in response.iter_content(chunk_size=1024):  # Leer en bloques de 1 KB
        if chunk:  # Ignorar fragmentos vac√≠os
            print(chunk.decode('utf-8'))  # Decodificar y procesar el fragmento
else:
    print(f"Error: {response.status_code}")
```
En este caso, los datos se procesan en fragmentos (`chunks`), lo que mejora la eficiencia y reduce el uso de recursos.

## 5. ¬øC√≥mo garantizar la escalabilidad de una API que consume modelos de IA pesados?
Para garantizar la escalabilidad de una API que consume modelos de IA pesados, se pueden implementar las siguientes estrategias:
- **Contenedores y orquestaci√≥n :** Usar Docker para contenerizar las instancias del modelo y Kubernetes para gestionar la escalabilidad horizontal.
- **Cach√© :** Implementar sistemas de cach√© (como Redis) para almacenar respuestas frecuentes y reducir la carga en los modelos.
- **Colas de mensajes :** Utilizar colas como RabbitMQ o Kafka para manejar solicitudes de manera asincr√≥nica y distribuir la carga.
- **Balanceo de carga :** Configurar un balanceador de carga (como NGINX o AWS ELB) para distribuir las solicitudes entre m√∫ltiples instancias del modelo.
- **Optimizaci√≥n de modelos :** Reducir el tama√±o del modelo mediante t√©cnicas como cuantizaci√≥n o pruning sin comprometer significativamente la calidad.
- **Autoscaling :** Implementar pol√≠ticas de escalado autom√°tico basadas en m√©tricas como CPU, memoria o latencia.

## 6. ¬øQu√© par√°metros de Ollama (ej: num_ctx, temperature) afectan el rendimiento/calidad de respuestas?
Algunos de los par√°metros clave de Ollama que afectan el rendimiento y la calidad de las respuestas incluyen:

- **num_ctx :** Define el tama√±o m√°ximo del contexto (en tokens) que el modelo puede considerar. Un valor m√°s alto permite respuestas m√°s contextualizadas pero aumenta el uso de recursos.
- **temperature :** Controla la aleatoriedad de las respuestas. Valores m√°s bajos (cerca de 0) generan respuestas m√°s deterministas y conservadoras, mientras que valores m√°s altos (cerca de 1 o superior) producen respuestas m√°s creativas pero menos predecibles.
- **top_k y top_p :** Filtran las opciones de salida del modelo. top_k limita el n√∫mero de palabras candidatas, mientras que top_p selecciona las palabras basadas en la probabilidad acumulativa.
- **max_tokens :** Define el n√∫mero m√°ximo de tokens que el modelo generar√° en la respuesta. Un valor m√°s alto permite respuestas m√°s largas pero puede aumentar el tiempo de procesamiento.

## 7. ¬øQu√© estrategias usar para balancear carga entre m√∫ltiples instancias de Ollama?
Para balancear la carga entre m√∫ltiples instancias de Ollama, se pueden implementar las siguientes estrategias:

- **Balanceador de carga :** Usar herramientas como NGINX, HAProxy o AWS Elastic Load Balancer para distribuir las solicitudes entre las instancias.
- **Orquestaci√≥n con Kubernetes :** Implementar Kubernetes para gestionar din√°micamente las instancias de Ollama, asegurando que las solicitudes se distribuyan equitativamente.
- **Colas de trabajo :** Utilizar sistemas de colas como Celery, RabbitMQ o Kafka para enviar solicitudes a diferentes instancias seg√∫n su disponibilidad.
- **Health checks :** Configurar verificaciones de salud para detectar instancias no disponibles y redirigir el tr√°fico solo a las instancias activas.
- **Sharding :** Dividir las solicitudes seg√∫n criterios espec√≠ficos (por ejemplo, por tipo de tarea o usuario) para asignarlas a instancias espec√≠ficas.

## 8. ¬øQu√© patrones de dise√±o (ej: CQRS, Singleton) son √∫tiles para integrar modelos de IA en backend?
Algunos patrones de dise√±o √∫tiles para integrar modelos de IA en backend incluyen:
- CQRS (Command Query Responsibility Segregation) : Separa las operaciones de lectura y escritura, permitiendo optimizar el procesamiento de consultas y comandos. Esto es √∫til cuando las consultas a modelos de IA son intensivas en recursos.
- **Singleton :** Asegura que solo exista una instancia del modelo de IA en memoria, reduciendo el consumo de recursos y mejorando la eficiencia.
- **Facade :** Proporciona una interfaz simplificada para interactuar con el modelo de IA, ocultando la complejidad de su implementaci√≥n.
- **Observer :** Permite que m√∫ltiples componentes del sistema reaccionen a eventos generados por el modelo de IA, como la finalizaci√≥n de una tarea.
- **Pipeline :** Divide el procesamiento en etapas secuenciales, lo que facilita la gesti√≥n de flujos de trabajo complejos, como la preprocesamiento de datos, inferencia y postprocesamiento.
- **Adapter :** Convierte la interfaz del modelo de IA en una interfaz compatible con el resto del sistema, facilitando su integraci√≥n.